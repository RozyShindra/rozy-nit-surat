    <section id="experience">
        <div class="container">
            <h2 class="section-title">Professional Experience</h2>
            <div class="experience-list">
                <div class="card experience-card">
                    <h3>Artificial Intelligence Intern</h3>
                    <h4>TATA Electronics Private Limited</h4>
                    <p class="location">Hosur, Tamil Nadu | March 2025 – June 2025</p>
                    <ul class="achievements">
                        <li>Material Process Optimization: Designed the SAM based model for UV images, achieved 95% glue application accuracy, reducing NG products by 30%. Enhanced gluing speed, leading to a 20% increase in process throughput</li>
                        <li>Computer Vision for Quality Control: Built a Computer vision-based system that reduced human error by 30% via automated defect detection. Boosted throughput to 6,000 parts/shift with 20% reduction in evaluation time.</li>
                        <li>Anodization Prediction Model: Developed ML models (Ensemble + Neural Nets) to predict color, gloss, texture, and thickness with up to 97% accuracy. Collaborated with cross-functional teams.</li>
                    </ul>
                </div>
                <div class="card experience-card">
                    <h3>Research & Development Intern</h3>
                    <h4>Toshiba Software (India) Private Limited</h4>
                    <p class="location">Bengaluru, Karnataka | July 2024 – Jan 2025</p>
                    <ul class="achievements">
                        <li>Proposed Architecture: Designed and implemented a scalable NLP architecture to optimize model performance and operational efficiency for information extraction. Improved information extraction accuracy by 15%, achieving an 80% F1 score, making the model more reliable for domain-specific tasks.</li>
                        <li>Advanced NLP Solutions: Delivered state-of-the-art systems for Named Entity Recognition (NER), Keyphrase Extraction (KPE), and Text Summarization, by leveraging LLMs (Llama, Mistral, Pegasus 10+ LLMs).</li>
                        <li>Fine-Tuning Expertise: Fine-tuned Large Language Models (LLMs) using Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA, Prefix Tuning, and Prompt Tuning for domain-specific applications. Benchmarking of multiple LLMs on a customized dataset, given metrics, and documentation of everything.</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>
    